{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dogbreed_Final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Y3AAQ36b6_u3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c444722e-7b47-443b-821d-868964f33728"
      },
      "cell_type": "code",
      "source": [
        "# Import kaggle.json from google drive\n",
        "# This snippet will output a link which needs authentication from any google account\n",
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "    q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "# print(kaggle_api_key)\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cZrVIoyh7ITX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!chmod 600 /content/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TPJmWZEv7LW9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FOrUJOVP7OeM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir dogsbreeds_final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MSCTjyU47ROX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "e3112119-d755-421e-90e0-a6961acdfa42"
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c dog-breed-identification -p /dogsbreeds_final/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading labels.csv.zip to /dogsbreeds_final\n",
            "\r  0% 0.00/214k [00:00<?, ?B/s]\n",
            "100% 214k/214k [00:00<00:00, 33.8MB/s]\n",
            "Downloading sample_submission.csv.zip to /dogsbreeds_final\n",
            "  0% 0.00/281k [00:00<?, ?B/s]\n",
            "100% 281k/281k [00:00<00:00, 38.1MB/s]\n",
            "Downloading test.zip to /dogsbreeds_final\n",
            "100% 346M/346M [00:03<00:00, 98.1MB/s]\n",
            "\n",
            "Downloading train.zip to /dogsbreeds_final\n",
            " 94% 324M/345M [00:02<00:00, 118MB/s]\n",
            "100% 345M/345M [00:02<00:00, 128MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pla327IG810S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir(\"/dogsbreeds_final/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TKs4stE28_Zk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "735630a7-658c-434b-a785-21b3ffb4a53d"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labels.csv.zip\tsample_submission.csv.zip  test.zip  train.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a7vd_oC09C_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "133f2428-9f14-45c0-909d-8c1270171223"
      },
      "cell_type": "code",
      "source": [
        "!unzip -q \\*.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "4 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xl2xXmIJ9IcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1e394d8c-a790-4be7-ba6e-378ca23cfa58"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labels.csv\tsample_submission.csv\t   test      train\n",
            "labels.csv.zip\tsample_submission.csv.zip  test.zip  train.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FsG03EMr9KZm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfe020ff-0881-4bd7-f4a4-f777ef1d41a6"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import skimage\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten,BatchNormalization,GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.applications.xception import Xception\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.applications import InceptionResNetV2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1tHOKJ0b9WaS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "PATH=\"/dogsbreeds_final/\"\n",
        "df = pd.read_csv(os.path.join(PATH,'labels.csv'))\n",
        "df_test = pd.read_csv(os.path.join(PATH,'sample_submission.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VuEK2F_Jyia_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Load, Resize and preprocess the images **"
      ]
    },
    {
      "metadata": {
        "id": "PdyITWeU9c3z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5ffdbf3b-313d-4d47-9503-b93729c485dc"
      },
      "cell_type": "code",
      "source": [
        "targets_series = pd.Series(df['breed'])\n",
        "one_hot = pd.get_dummies(targets_series, sparse = True)\n",
        "one_hot_labels = np.asarray(one_hot)\n",
        "\n",
        "im_size = 32\n",
        "x_train1 = []\n",
        "y_train1 = []\n",
        "x_test1 = []\n",
        "i = 0 \n",
        "new_shape = (128,128,3)\n",
        "\n",
        "for f, breed in tqdm(df.values):\n",
        "    img = cv2.imread('/dogsbreeds_final/train/{}.jpg'.format(f))\n",
        "    label = one_hot_labels[i]\n",
        "    #print(label)\n",
        "    #x_train1.append(cv2.resize(img, (im_size, im_size)))\n",
        "    X_data_resized = resize(img, new_shape)\n",
        "    x_train1.append(X_data_resized)\n",
        "    #res = tf.keras.backend.resize_images(img, 32, 32, \"channels_last\",interpolation='bilinear')\n",
        "    y_train1.append(label)\n",
        "    i += 1\n",
        "  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10222 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "100%|██████████| 10222/10222 [02:47<00:00, 61.13it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "7VwGgsfw_Fjh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0cd5a94a-094d-4f86-b1a3-3cb067b77cd9"
      },
      "cell_type": "code",
      "source": [
        "#new_shape = (32,32,3)\n",
        "\n",
        "y_train_raw = np.array(y_train1, np.uint8)\n",
        "x_train_raw = np.array(x_train1, np.float32) / 255.\n",
        "x_testContest  = np.array(x_test1, np.float32) / 255.\n",
        "\n",
        "num_class = y_train_raw.shape[1]\n",
        "\n",
        "print(x_train_raw.shape)\n",
        "print(y_train_raw.shape)   "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10222, 128, 128, 3)\n",
            "(10222, 120)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KJQRo00NytvA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Split Data into Train and Test**"
      ]
    },
    {
      "metadata": {
        "id": "RdqKZKX1_VTe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x_train_raw, y_train_raw, test_size=0.3, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0F4PTZQD_d8Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 20\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7vL-VEt3D7i6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Best performing Model**"
      ]
    },
    {
      "metadata": {
        "id": "9LsRuRsS_llq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "outputId": "81f857dc-4c7a-419c-a5a3-41940c05dce7"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(128, 128, 3)))\n",
        "model.add(Conv2D(filters=16, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=32, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=64, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=128, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=256, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(num_class, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization_31 (Batc (None, 128, 128, 3)       12        \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 126, 126, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 63, 63, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 63, 63, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 61, 61, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 30, 30, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 28, 28, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_5 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 120)               30840     \n",
            "=================================================================\n",
            "Total params: 425,444\n",
            "Trainable params: 424,446\n",
            "Non-trainable params: 998\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0uMNCwjhAIdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "3a38e715-4979-4234-d027-6aed9185a2c0"
      },
      "cell_type": "code",
      "source": [
        "opt = optimizers.Adam(lr = learning_rate)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
        "                  batch_size=batch_size,verbose=2, epochs=epochs, shuffle = True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7155 samples, validate on 3067 samples\n",
            "Epoch 1/20\n",
            " - 10s - loss: 4.6109 - acc: 0.0366 - val_loss: 4.5235 - val_acc: 0.0525\n",
            "Epoch 2/20\n",
            " - 8s - loss: 4.0289 - acc: 0.1019 - val_loss: 4.2774 - val_acc: 0.0717\n",
            "Epoch 3/20\n",
            " - 8s - loss: 3.6494 - acc: 0.1757 - val_loss: 4.1586 - val_acc: 0.0867\n",
            "Epoch 4/20\n",
            " - 7s - loss: 3.2650 - acc: 0.2738 - val_loss: 4.1234 - val_acc: 0.0910\n",
            "Epoch 5/20\n",
            " - 7s - loss: 2.8647 - acc: 0.3782 - val_loss: 4.1364 - val_acc: 0.0978\n",
            "Epoch 6/20\n",
            " - 7s - loss: 2.4166 - acc: 0.5110 - val_loss: 4.0632 - val_acc: 0.1017\n",
            "Epoch 7/20\n",
            " - 7s - loss: 1.9388 - acc: 0.6477 - val_loss: 4.1734 - val_acc: 0.0890\n",
            "Epoch 8/20\n",
            " - 7s - loss: 1.4647 - acc: 0.7740 - val_loss: 4.1758 - val_acc: 0.0871\n",
            "Epoch 9/20\n",
            " - 7s - loss: 1.0033 - acc: 0.8839 - val_loss: 4.1667 - val_acc: 0.0975\n",
            "Epoch 10/20\n",
            " - 7s - loss: 0.6293 - acc: 0.9533 - val_loss: 4.3042 - val_acc: 0.0919\n",
            "Epoch 11/20\n",
            " - 7s - loss: 0.3631 - acc: 0.9836 - val_loss: 4.3226 - val_acc: 0.0942\n",
            "Epoch 12/20\n",
            " - 7s - loss: 0.1986 - acc: 0.9945 - val_loss: 4.3181 - val_acc: 0.0985\n",
            "Epoch 13/20\n",
            " - 7s - loss: 0.1073 - acc: 0.9986 - val_loss: 4.3909 - val_acc: 0.1004\n",
            "Epoch 14/20\n",
            " - 7s - loss: 0.0726 - acc: 0.9990 - val_loss: 4.3891 - val_acc: 0.0975\n",
            "Epoch 15/20\n",
            " - 8s - loss: 0.0515 - acc: 0.9996 - val_loss: 4.3718 - val_acc: 0.1004\n",
            "Epoch 16/20\n",
            " - 8s - loss: 0.0388 - acc: 0.9996 - val_loss: 4.4092 - val_acc: 0.1017\n",
            "Epoch 17/20\n",
            " - 8s - loss: 0.0358 - acc: 0.9993 - val_loss: 4.4201 - val_acc: 0.1037\n",
            "Epoch 18/20\n",
            " - 8s - loss: 0.0282 - acc: 0.9994 - val_loss: 4.4626 - val_acc: 0.1060\n",
            "Epoch 19/20\n",
            " - 8s - loss: 0.0269 - acc: 0.9996 - val_loss: 4.4784 - val_acc: 0.1050\n",
            "Epoch 20/20\n",
            " - 8s - loss: 0.0226 - acc: 0.9994 - val_loss: 4.5021 - val_acc: 0.1063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kUxZxzTEy8N6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Best model score and loss**"
      ]
    },
    {
      "metadata": {
        "id": "xLIIN2KUDQDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8c9af4f6-e341-427d-ad90-ecb835804567"
      },
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', loss)\n",
        "print('Test acc:', acc*100)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 4.5020759876053456\n",
            "Test acc: 10.629279427121041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kjwR5Zu5DaR1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "fb7f31d1-7d41-4af7-b02a-214276025d23"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nV8a8cR4FQAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "KwTfZtsWDmd2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "0d97cb8d-6fef-4036-f72a-19e82f6d9f36"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFnCAYAAACcvYGMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "xWAoCYupbBbE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Pre-trained Model Xception**"
      ]
    },
    {
      "metadata": {
        "id": "LIVhvB4zctyI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4842
        },
        "outputId": "e82a67ab-6457-4428-b110-6a1eee310799"
      },
      "cell_type": "code",
      "source": [
        "nTrain = 10000\n",
        "#datagen = ImageGenerator(rescale=1.0/255)\n",
        "#train_features  = np.zeros()\n",
        "base_model = Xception(weights='imagenet',include_top= False ,input_shape=(128,128,3))\n",
        "# Model structure\n",
        "base_model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 63, 63, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 63, 63, 32)   128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 63, 63, 32)   0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 61, 61, 64)   18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 61, 61, 64)   256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 61, 61, 64)   0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 61, 61, 128)  8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 61, 61, 128)  512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 61, 61, 128)  0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 61, 61, 128)  17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 61, 61, 128)  512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 31, 31, 128)  8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 31, 31, 128)  0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 31, 31, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 31, 31, 128)  0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 31, 31, 128)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 31, 31, 256)  33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 31, 31, 256)  1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 31, 31, 256)  0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 31, 31, 256)  67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 31, 31, 256)  1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 256)  32768       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 16, 16, 256)  0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 256)  1024        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 16, 256)  0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 16, 16, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 16, 16, 728)  188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 16, 16, 728)  0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 8, 8, 728)    186368      add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 8, 8, 728)    0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 8, 8, 728)    2912        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 8, 8, 728)    0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 8, 8, 728)    0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 8, 8, 728)    0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 8, 8, 728)    0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 728)    0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 8, 8, 728)    0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 8, 8, 728)    0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 8, 8, 728)    0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 728)    0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 8, 8, 728)    0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 8, 8, 728)    0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 8, 8, 728)    0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 728)    0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 8, 8, 728)    0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 8, 8, 728)    0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 8, 8, 728)    0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 728)    0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 8, 8, 728)    0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 8, 8, 728)    0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 8, 8, 728)    0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 728)    0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 8, 8, 728)    0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 8, 8, 728)    536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 8, 8, 728)    2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 8, 8, 728)    0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 8, 8, 728)    536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 8, 8, 728)    2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 8, 8, 728)    0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 8, 8, 728)    536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 8, 8, 728)    2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 8, 8, 728)    0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 8, 8, 728)    0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 8, 8, 728)    536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 8, 8, 728)    2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 8, 8, 728)    0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 8, 8, 728)    536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 8, 8, 728)    2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 8, 8, 728)    0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 8, 8, 728)    536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 8, 8, 728)    2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 8, 8, 728)    0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 8, 8, 728)    0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 8, 8, 728)    536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 8, 8, 728)    2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 8, 8, 728)    0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 8, 8, 728)    536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 8, 8, 728)    2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 8, 8, 728)    0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 8, 8, 728)    536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 8, 8, 728)    2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 8, 8, 728)    0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 8, 8, 728)    0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 8, 8, 728)    536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 8, 8, 728)    2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 8, 8, 728)    0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 8, 8, 1024)   752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 8, 8, 1024)   4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 4, 4, 1024)   745472      add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 4, 4, 1024)   0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 4, 4, 1024)   4096        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 4, 4, 1024)   0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 4, 4, 1536)   1582080     add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 4, 4, 1536)   6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 4, 4, 1536)   0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 4, 4, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 4, 4, 2048)   8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 4, 4, 2048)   0           block14_sepconv2_bn[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 20,861,480\n",
            "Trainable params: 20,806,952\n",
            "Non-trainable params: 54,528\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yPo5f-Vod0kJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Adding DogBreed data to Xception model**"
      ]
    },
    {
      "metadata": {
        "id": "CGhUp96LdN1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1339b4ef-0700-4001-e3a2-1219dea2c24e"
      },
      "cell_type": "code",
      "source": [
        "x_train = base_model.predict(x_train)\n",
        "x_val = base_model.predict(x_test)\n",
        "print(x_train.shape)\n",
        "print(x_val.shape)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7155, 4, 4, 2048)\n",
            "(3067, 4, 4, 2048)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mNnfFYiSeNpD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(512))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(num_class,activation='softmax'))\n",
        "opt = optimizers.SGD(lr=.0001)\n",
        "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FedxoqwAeih6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5144
        },
        "outputId": "d280cb9d-a0b1-4d26-ba8b-fb2b0b7f5a30"
      },
      "cell_type": "code",
      "source": [
        "fit_m =model.fit(x_train,y_train, batch_size=32, epochs=150,validation_data=(x_val,y_test),shuffle=True)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train on 7155 samples, validate on 3067 samples\n",
            "Epoch 1/150\n",
            "7155/7155 [==============================] - 7s 1ms/step - loss: 4.7892 - acc: 0.0087 - val_loss: 4.7887 - val_acc: 0.0062\n",
            "Epoch 2/150\n",
            "7155/7155 [==============================] - 4s 555us/step - loss: 4.7899 - acc: 0.0085 - val_loss: 4.7887 - val_acc: 0.0072\n",
            "Epoch 3/150\n",
            "7155/7155 [==============================] - 4s 557us/step - loss: 4.7898 - acc: 0.0075 - val_loss: 4.7886 - val_acc: 0.0078\n",
            "Epoch 4/150\n",
            "7155/7155 [==============================] - 4s 563us/step - loss: 4.7892 - acc: 0.0084 - val_loss: 4.7886 - val_acc: 0.0082\n",
            "Epoch 5/150\n",
            "7155/7155 [==============================] - 4s 563us/step - loss: 4.7896 - acc: 0.0094 - val_loss: 4.7885 - val_acc: 0.0082\n",
            "Epoch 6/150\n",
            "7155/7155 [==============================] - 4s 573us/step - loss: 4.7895 - acc: 0.0082 - val_loss: 4.7885 - val_acc: 0.0091\n",
            "Epoch 7/150\n",
            "7155/7155 [==============================] - 4s 571us/step - loss: 4.7900 - acc: 0.0073 - val_loss: 4.7885 - val_acc: 0.0095\n",
            "Epoch 8/150\n",
            "7155/7155 [==============================] - 4s 574us/step - loss: 4.7893 - acc: 0.0077 - val_loss: 4.7884 - val_acc: 0.0095\n",
            "Epoch 9/150\n",
            "7155/7155 [==============================] - 4s 575us/step - loss: 4.7894 - acc: 0.0061 - val_loss: 4.7884 - val_acc: 0.0091\n",
            "Epoch 10/150\n",
            "7155/7155 [==============================] - 4s 562us/step - loss: 4.7893 - acc: 0.0082 - val_loss: 4.7883 - val_acc: 0.0091\n",
            "Epoch 11/150\n",
            "7155/7155 [==============================] - 4s 565us/step - loss: 4.7897 - acc: 0.0084 - val_loss: 4.7883 - val_acc: 0.0088\n",
            "Epoch 12/150\n",
            "7155/7155 [==============================] - 4s 565us/step - loss: 4.7892 - acc: 0.0063 - val_loss: 4.7883 - val_acc: 0.0088\n",
            "Epoch 13/150\n",
            "7155/7155 [==============================] - 4s 558us/step - loss: 4.7886 - acc: 0.0082 - val_loss: 4.7882 - val_acc: 0.0091\n",
            "Epoch 14/150\n",
            "7155/7155 [==============================] - 4s 553us/step - loss: 4.7888 - acc: 0.0066 - val_loss: 4.7882 - val_acc: 0.0091\n",
            "Epoch 15/150\n",
            "7155/7155 [==============================] - 4s 561us/step - loss: 4.7889 - acc: 0.0092 - val_loss: 4.7882 - val_acc: 0.0091\n",
            "Epoch 16/150\n",
            "7155/7155 [==============================] - 4s 568us/step - loss: 4.7891 - acc: 0.0080 - val_loss: 4.7881 - val_acc: 0.0088\n",
            "Epoch 17/150\n",
            "7155/7155 [==============================] - 4s 568us/step - loss: 4.7891 - acc: 0.0073 - val_loss: 4.7881 - val_acc: 0.0085\n",
            "Epoch 18/150\n",
            "7155/7155 [==============================] - 4s 564us/step - loss: 4.7892 - acc: 0.0081 - val_loss: 4.7880 - val_acc: 0.0088\n",
            "Epoch 19/150\n",
            "7155/7155 [==============================] - 4s 564us/step - loss: 4.7893 - acc: 0.0075 - val_loss: 4.7880 - val_acc: 0.0088\n",
            "Epoch 20/150\n",
            "7155/7155 [==============================] - 4s 571us/step - loss: 4.7884 - acc: 0.0091 - val_loss: 4.7880 - val_acc: 0.0088\n",
            "Epoch 21/150\n",
            "7155/7155 [==============================] - 4s 566us/step - loss: 4.7890 - acc: 0.0082 - val_loss: 4.7879 - val_acc: 0.0091\n",
            "Epoch 22/150\n",
            "7155/7155 [==============================] - 4s 552us/step - loss: 4.7887 - acc: 0.0105 - val_loss: 4.7879 - val_acc: 0.0091\n",
            "Epoch 23/150\n",
            "7155/7155 [==============================] - 4s 556us/step - loss: 4.7883 - acc: 0.0077 - val_loss: 4.7879 - val_acc: 0.0088\n",
            "Epoch 24/150\n",
            "7155/7155 [==============================] - 4s 540us/step - loss: 4.7883 - acc: 0.0094 - val_loss: 4.7878 - val_acc: 0.0088\n",
            "Epoch 25/150\n",
            "7155/7155 [==============================] - 3s 479us/step - loss: 4.7876 - acc: 0.0085 - val_loss: 4.7878 - val_acc: 0.0088\n",
            "Epoch 26/150\n",
            "7155/7155 [==============================] - 3s 469us/step - loss: 4.7888 - acc: 0.0089 - val_loss: 4.7878 - val_acc: 0.0088\n",
            "Epoch 27/150\n",
            "7155/7155 [==============================] - 3s 463us/step - loss: 4.7881 - acc: 0.0082 - val_loss: 4.7877 - val_acc: 0.0091\n",
            "Epoch 28/150\n",
            "7155/7155 [==============================] - 3s 468us/step - loss: 4.7883 - acc: 0.0081 - val_loss: 4.7877 - val_acc: 0.0095\n",
            "Epoch 29/150\n",
            "7155/7155 [==============================] - 3s 469us/step - loss: 4.7882 - acc: 0.0089 - val_loss: 4.7877 - val_acc: 0.0095\n",
            "Epoch 30/150\n",
            "7155/7155 [==============================] - 3s 468us/step - loss: 4.7883 - acc: 0.0085 - val_loss: 4.7876 - val_acc: 0.0098\n",
            "Epoch 31/150\n",
            "7155/7155 [==============================] - 3s 468us/step - loss: 4.7879 - acc: 0.0088 - val_loss: 4.7876 - val_acc: 0.0095\n",
            "Epoch 32/150\n",
            "7155/7155 [==============================] - 3s 470us/step - loss: 4.7881 - acc: 0.0095 - val_loss: 4.7876 - val_acc: 0.0095\n",
            "Epoch 33/150\n",
            "7155/7155 [==============================] - 3s 462us/step - loss: 4.7876 - acc: 0.0078 - val_loss: 4.7875 - val_acc: 0.0098\n",
            "Epoch 34/150\n",
            "7155/7155 [==============================] - 3s 460us/step - loss: 4.7882 - acc: 0.0077 - val_loss: 4.7875 - val_acc: 0.0101\n",
            "Epoch 35/150\n",
            "7155/7155 [==============================] - 3s 465us/step - loss: 4.7879 - acc: 0.0078 - val_loss: 4.7875 - val_acc: 0.0104\n",
            "Epoch 36/150\n",
            "7155/7155 [==============================] - 3s 461us/step - loss: 4.7887 - acc: 0.0084 - val_loss: 4.7874 - val_acc: 0.0108\n",
            "Epoch 37/150\n",
            "7155/7155 [==============================] - 3s 461us/step - loss: 4.7880 - acc: 0.0094 - val_loss: 4.7874 - val_acc: 0.0108\n",
            "Epoch 38/150\n",
            "7155/7155 [==============================] - 3s 454us/step - loss: 4.7880 - acc: 0.0081 - val_loss: 4.7874 - val_acc: 0.0111\n",
            "Epoch 39/150\n",
            "7155/7155 [==============================] - 3s 454us/step - loss: 4.7878 - acc: 0.0094 - val_loss: 4.7874 - val_acc: 0.0101\n",
            "Epoch 40/150\n",
            "7155/7155 [==============================] - 3s 452us/step - loss: 4.7878 - acc: 0.0103 - val_loss: 4.7873 - val_acc: 0.0101\n",
            "Epoch 41/150\n",
            "7155/7155 [==============================] - 3s 452us/step - loss: 4.7878 - acc: 0.0091 - val_loss: 4.7873 - val_acc: 0.0104\n",
            "Epoch 42/150\n",
            "7155/7155 [==============================] - 3s 456us/step - loss: 4.7877 - acc: 0.0096 - val_loss: 4.7873 - val_acc: 0.0108\n",
            "Epoch 43/150\n",
            "7155/7155 [==============================] - 3s 453us/step - loss: 4.7876 - acc: 0.0092 - val_loss: 4.7872 - val_acc: 0.0104\n",
            "Epoch 44/150\n",
            "7155/7155 [==============================] - 3s 453us/step - loss: 4.7879 - acc: 0.0081 - val_loss: 4.7872 - val_acc: 0.0101\n",
            "Epoch 45/150\n",
            "7155/7155 [==============================] - 3s 456us/step - loss: 4.7878 - acc: 0.0091 - val_loss: 4.7872 - val_acc: 0.0101\n",
            "Epoch 46/150\n",
            "7155/7155 [==============================] - 3s 454us/step - loss: 4.7877 - acc: 0.0080 - val_loss: 4.7872 - val_acc: 0.0101\n",
            "Epoch 47/150\n",
            "7155/7155 [==============================] - 3s 455us/step - loss: 4.7875 - acc: 0.0092 - val_loss: 4.7871 - val_acc: 0.0101\n",
            "Epoch 48/150\n",
            "7155/7155 [==============================] - 4s 519us/step - loss: 4.7877 - acc: 0.0075 - val_loss: 4.7871 - val_acc: 0.0101\n",
            "Epoch 49/150\n",
            "7155/7155 [==============================] - 4s 611us/step - loss: 4.7876 - acc: 0.0087 - val_loss: 4.7871 - val_acc: 0.0101\n",
            "Epoch 50/150\n",
            "7155/7155 [==============================] - 5s 635us/step - loss: 4.7878 - acc: 0.0095 - val_loss: 4.7870 - val_acc: 0.0104\n",
            "Epoch 51/150\n",
            "7155/7155 [==============================] - 4s 614us/step - loss: 4.7875 - acc: 0.0098 - val_loss: 4.7870 - val_acc: 0.0104\n",
            "Epoch 52/150\n",
            "7155/7155 [==============================] - 4s 611us/step - loss: 4.7872 - acc: 0.0091 - val_loss: 4.7870 - val_acc: 0.0104\n",
            "Epoch 53/150\n",
            "7155/7155 [==============================] - 4s 606us/step - loss: 4.7873 - acc: 0.0102 - val_loss: 4.7870 - val_acc: 0.0104\n",
            "Epoch 54/150\n",
            "7155/7155 [==============================] - 5s 636us/step - loss: 4.7870 - acc: 0.0078 - val_loss: 4.7869 - val_acc: 0.0104\n",
            "Epoch 55/150\n",
            "7155/7155 [==============================] - 4s 599us/step - loss: 4.7870 - acc: 0.0094 - val_loss: 4.7869 - val_acc: 0.0104\n",
            "Epoch 56/150\n",
            "7155/7155 [==============================] - 4s 624us/step - loss: 4.7876 - acc: 0.0091 - val_loss: 4.7869 - val_acc: 0.0101\n",
            "Epoch 57/150\n",
            "7155/7155 [==============================] - 4s 612us/step - loss: 4.7874 - acc: 0.0098 - val_loss: 4.7869 - val_acc: 0.0101\n",
            "Epoch 58/150\n",
            "7155/7155 [==============================] - 4s 624us/step - loss: 4.7870 - acc: 0.0095 - val_loss: 4.7868 - val_acc: 0.0101\n",
            "Epoch 59/150\n",
            "7155/7155 [==============================] - 5s 632us/step - loss: 4.7873 - acc: 0.0094 - val_loss: 4.7868 - val_acc: 0.0101\n",
            "Epoch 60/150\n",
            "7155/7155 [==============================] - 4s 615us/step - loss: 4.7872 - acc: 0.0101 - val_loss: 4.7868 - val_acc: 0.0101\n",
            "Epoch 61/150\n",
            "7155/7155 [==============================] - 5s 635us/step - loss: 4.7870 - acc: 0.0102 - val_loss: 4.7867 - val_acc: 0.0101\n",
            "Epoch 62/150\n",
            "7155/7155 [==============================] - 5s 641us/step - loss: 4.7875 - acc: 0.0096 - val_loss: 4.7867 - val_acc: 0.0101\n",
            "Epoch 63/150\n",
            "7155/7155 [==============================] - 5s 645us/step - loss: 4.7866 - acc: 0.0112 - val_loss: 4.7867 - val_acc: 0.0101\n",
            "Epoch 64/150\n",
            "7155/7155 [==============================] - 5s 630us/step - loss: 4.7866 - acc: 0.0089 - val_loss: 4.7867 - val_acc: 0.0101\n",
            "Epoch 65/150\n",
            "7155/7155 [==============================] - 5s 638us/step - loss: 4.7869 - acc: 0.0108 - val_loss: 4.7866 - val_acc: 0.0101\n",
            "Epoch 66/150\n",
            "7155/7155 [==============================] - 5s 631us/step - loss: 4.7868 - acc: 0.0117 - val_loss: 4.7866 - val_acc: 0.0101\n",
            "Epoch 67/150\n",
            "7155/7155 [==============================] - 5s 635us/step - loss: 4.7867 - acc: 0.0110 - val_loss: 4.7866 - val_acc: 0.0101\n",
            "Epoch 68/150\n",
            "7155/7155 [==============================] - 4s 624us/step - loss: 4.7862 - acc: 0.0126 - val_loss: 4.7866 - val_acc: 0.0101\n",
            "Epoch 69/150\n",
            "7155/7155 [==============================] - 5s 632us/step - loss: 4.7868 - acc: 0.0110 - val_loss: 4.7865 - val_acc: 0.0101\n",
            "Epoch 70/150\n",
            "7155/7155 [==============================] - 4s 626us/step - loss: 4.7867 - acc: 0.0098 - val_loss: 4.7865 - val_acc: 0.0101\n",
            "Epoch 71/150\n",
            "7155/7155 [==============================] - 4s 620us/step - loss: 4.7867 - acc: 0.0095 - val_loss: 4.7865 - val_acc: 0.0101\n",
            "Epoch 72/150\n",
            "7155/7155 [==============================] - 5s 644us/step - loss: 4.7867 - acc: 0.0098 - val_loss: 4.7865 - val_acc: 0.0104\n",
            "Epoch 73/150\n",
            "7155/7155 [==============================] - 4s 623us/step - loss: 4.7866 - acc: 0.0112 - val_loss: 4.7864 - val_acc: 0.0104\n",
            "Epoch 74/150\n",
            "7155/7155 [==============================] - 4s 628us/step - loss: 4.7861 - acc: 0.0109 - val_loss: 4.7864 - val_acc: 0.0104\n",
            "Epoch 75/150\n",
            "7155/7155 [==============================] - 4s 625us/step - loss: 4.7867 - acc: 0.0127 - val_loss: 4.7864 - val_acc: 0.0101\n",
            "Epoch 76/150\n",
            "7155/7155 [==============================] - 4s 621us/step - loss: 4.7865 - acc: 0.0117 - val_loss: 4.7864 - val_acc: 0.0101\n",
            "Epoch 77/150\n",
            "7155/7155 [==============================] - 4s 596us/step - loss: 4.7866 - acc: 0.0084 - val_loss: 4.7863 - val_acc: 0.0101\n",
            "Epoch 78/150\n",
            "7155/7155 [==============================] - 4s 576us/step - loss: 4.7866 - acc: 0.0099 - val_loss: 4.7863 - val_acc: 0.0104\n",
            "Epoch 79/150\n",
            "7155/7155 [==============================] - 4s 536us/step - loss: 4.7860 - acc: 0.0117 - val_loss: 4.7863 - val_acc: 0.0104\n",
            "Epoch 80/150\n",
            "7155/7155 [==============================] - 4s 522us/step - loss: 4.7861 - acc: 0.0108 - val_loss: 4.7863 - val_acc: 0.0104\n",
            "Epoch 81/150\n",
            "7155/7155 [==============================] - 4s 510us/step - loss: 4.7862 - acc: 0.0088 - val_loss: 4.7863 - val_acc: 0.0108\n",
            "Epoch 82/150\n",
            "7155/7155 [==============================] - 4s 547us/step - loss: 4.7859 - acc: 0.0120 - val_loss: 4.7862 - val_acc: 0.0108\n",
            "Epoch 83/150\n",
            "7155/7155 [==============================] - 4s 559us/step - loss: 4.7857 - acc: 0.0099 - val_loss: 4.7862 - val_acc: 0.0114\n",
            "Epoch 84/150\n",
            "7155/7155 [==============================] - 4s 522us/step - loss: 4.7861 - acc: 0.0103 - val_loss: 4.7862 - val_acc: 0.0111\n",
            "Epoch 85/150\n",
            "7155/7155 [==============================] - 4s 508us/step - loss: 4.7862 - acc: 0.0080 - val_loss: 4.7862 - val_acc: 0.0108\n",
            "Epoch 86/150\n",
            "7155/7155 [==============================] - 4s 503us/step - loss: 4.7862 - acc: 0.0119 - val_loss: 4.7861 - val_acc: 0.0121\n",
            "Epoch 87/150\n",
            "7155/7155 [==============================] - 4s 500us/step - loss: 4.7855 - acc: 0.0099 - val_loss: 4.7861 - val_acc: 0.0124\n",
            "Epoch 88/150\n",
            "7155/7155 [==============================] - 4s 498us/step - loss: 4.7858 - acc: 0.0110 - val_loss: 4.7861 - val_acc: 0.0124\n",
            "Epoch 89/150\n",
            "7155/7155 [==============================] - 4s 570us/step - loss: 4.7856 - acc: 0.0109 - val_loss: 4.7861 - val_acc: 0.0124\n",
            "Epoch 90/150\n",
            "7155/7155 [==============================] - 4s 577us/step - loss: 4.7862 - acc: 0.0115 - val_loss: 4.7860 - val_acc: 0.0127\n",
            "Epoch 91/150\n",
            "7155/7155 [==============================] - 4s 604us/step - loss: 4.7860 - acc: 0.0108 - val_loss: 4.7860 - val_acc: 0.0124\n",
            "Epoch 92/150\n",
            "7155/7155 [==============================] - 4s 612us/step - loss: 4.7858 - acc: 0.0103 - val_loss: 4.7860 - val_acc: 0.0124\n",
            "Epoch 93/150\n",
            "7155/7155 [==============================] - 4s 530us/step - loss: 4.7859 - acc: 0.0117 - val_loss: 4.7860 - val_acc: 0.0124\n",
            "Epoch 94/150\n",
            "7155/7155 [==============================] - 4s 552us/step - loss: 4.7855 - acc: 0.0130 - val_loss: 4.7860 - val_acc: 0.0124\n",
            "Epoch 95/150\n",
            "7155/7155 [==============================] - 4s 507us/step - loss: 4.7857 - acc: 0.0122 - val_loss: 4.7859 - val_acc: 0.0127\n",
            "Epoch 96/150\n",
            "7155/7155 [==============================] - 4s 550us/step - loss: 4.7855 - acc: 0.0112 - val_loss: 4.7859 - val_acc: 0.0124\n",
            "Epoch 97/150\n",
            "7155/7155 [==============================] - 4s 538us/step - loss: 4.7855 - acc: 0.0101 - val_loss: 4.7859 - val_acc: 0.0130\n",
            "Epoch 98/150\n",
            "7155/7155 [==============================] - 4s 541us/step - loss: 4.7855 - acc: 0.0110 - val_loss: 4.7859 - val_acc: 0.0134\n",
            "Epoch 99/150\n",
            "7155/7155 [==============================] - 4s 530us/step - loss: 4.7861 - acc: 0.0110 - val_loss: 4.7858 - val_acc: 0.0140\n",
            "Epoch 100/150\n",
            "7155/7155 [==============================] - 4s 533us/step - loss: 4.7852 - acc: 0.0108 - val_loss: 4.7858 - val_acc: 0.0147\n",
            "Epoch 101/150\n",
            "7155/7155 [==============================] - 4s 515us/step - loss: 4.7854 - acc: 0.0109 - val_loss: 4.7858 - val_acc: 0.0150\n",
            "Epoch 102/150\n",
            "7155/7155 [==============================] - 4s 516us/step - loss: 4.7854 - acc: 0.0116 - val_loss: 4.7858 - val_acc: 0.0150\n",
            "Epoch 103/150\n",
            "7155/7155 [==============================] - 4s 501us/step - loss: 4.7855 - acc: 0.0112 - val_loss: 4.7858 - val_acc: 0.0147\n",
            "Epoch 104/150\n",
            "7155/7155 [==============================] - 4s 496us/step - loss: 4.7850 - acc: 0.0095 - val_loss: 4.7857 - val_acc: 0.0143\n",
            "Epoch 105/150\n",
            "7155/7155 [==============================] - 4s 495us/step - loss: 4.7853 - acc: 0.0091 - val_loss: 4.7857 - val_acc: 0.0140\n",
            "Epoch 106/150\n",
            "7155/7155 [==============================] - 4s 496us/step - loss: 4.7851 - acc: 0.0108 - val_loss: 4.7857 - val_acc: 0.0137\n",
            "Epoch 107/150\n",
            "7155/7155 [==============================] - 4s 502us/step - loss: 4.7852 - acc: 0.0098 - val_loss: 4.7857 - val_acc: 0.0134\n",
            "Epoch 108/150\n",
            "7155/7155 [==============================] - 4s 500us/step - loss: 4.7856 - acc: 0.0106 - val_loss: 4.7857 - val_acc: 0.0134\n",
            "Epoch 109/150\n",
            "7155/7155 [==============================] - 4s 500us/step - loss: 4.7857 - acc: 0.0108 - val_loss: 4.7856 - val_acc: 0.0134\n",
            "Epoch 110/150\n",
            "7155/7155 [==============================] - 4s 498us/step - loss: 4.7851 - acc: 0.0110 - val_loss: 4.7856 - val_acc: 0.0134\n",
            "Epoch 111/150\n",
            "7155/7155 [==============================] - 4s 498us/step - loss: 4.7851 - acc: 0.0095 - val_loss: 4.7856 - val_acc: 0.0134\n",
            "Epoch 112/150\n",
            "7155/7155 [==============================] - 4s 496us/step - loss: 4.7849 - acc: 0.0105 - val_loss: 4.7856 - val_acc: 0.0134\n",
            "Epoch 113/150\n",
            "7155/7155 [==============================] - 4s 504us/step - loss: 4.7852 - acc: 0.0096 - val_loss: 4.7856 - val_acc: 0.0134\n",
            "Epoch 114/150\n",
            "7155/7155 [==============================] - 4s 499us/step - loss: 4.7848 - acc: 0.0120 - val_loss: 4.7855 - val_acc: 0.0137\n",
            "Epoch 115/150\n",
            "7155/7155 [==============================] - 4s 507us/step - loss: 4.7849 - acc: 0.0098 - val_loss: 4.7855 - val_acc: 0.0137\n",
            "Epoch 116/150\n",
            "7155/7155 [==============================] - 4s 512us/step - loss: 4.7850 - acc: 0.0110 - val_loss: 4.7855 - val_acc: 0.0137\n",
            "Epoch 117/150\n",
            "7155/7155 [==============================] - 4s 495us/step - loss: 4.7848 - acc: 0.0131 - val_loss: 4.7855 - val_acc: 0.0137\n",
            "Epoch 118/150\n",
            "7155/7155 [==============================] - 4s 492us/step - loss: 4.7848 - acc: 0.0108 - val_loss: 4.7855 - val_acc: 0.0137\n",
            "Epoch 119/150\n",
            "7155/7155 [==============================] - 4s 560us/step - loss: 4.7847 - acc: 0.0116 - val_loss: 4.7854 - val_acc: 0.0137\n",
            "Epoch 120/150\n",
            "7155/7155 [==============================] - 4s 506us/step - loss: 4.7844 - acc: 0.0119 - val_loss: 4.7854 - val_acc: 0.0137\n",
            "Epoch 121/150\n",
            "7155/7155 [==============================] - 4s 527us/step - loss: 4.7848 - acc: 0.0117 - val_loss: 4.7854 - val_acc: 0.0137\n",
            "Epoch 122/150\n",
            "7155/7155 [==============================] - 4s 530us/step - loss: 4.7846 - acc: 0.0117 - val_loss: 4.7854 - val_acc: 0.0137\n",
            "Epoch 123/150\n",
            "7155/7155 [==============================] - 4s 533us/step - loss: 4.7850 - acc: 0.0117 - val_loss: 4.7854 - val_acc: 0.0137\n",
            "Epoch 124/150\n",
            "7155/7155 [==============================] - 4s 536us/step - loss: 4.7853 - acc: 0.0103 - val_loss: 4.7853 - val_acc: 0.0137\n",
            "Epoch 125/150\n",
            "7155/7155 [==============================] - 4s 534us/step - loss: 4.7845 - acc: 0.0119 - val_loss: 4.7853 - val_acc: 0.0137\n",
            "Epoch 126/150\n",
            "7155/7155 [==============================] - 4s 544us/step - loss: 4.7849 - acc: 0.0088 - val_loss: 4.7853 - val_acc: 0.0137\n",
            "Epoch 127/150\n",
            "7155/7155 [==============================] - 4s 559us/step - loss: 4.7845 - acc: 0.0123 - val_loss: 4.7853 - val_acc: 0.0137\n",
            "Epoch 128/150\n",
            "7155/7155 [==============================] - 4s 546us/step - loss: 4.7843 - acc: 0.0120 - val_loss: 4.7853 - val_acc: 0.0137\n",
            "Epoch 129/150\n",
            "7155/7155 [==============================] - 4s 556us/step - loss: 4.7843 - acc: 0.0117 - val_loss: 4.7852 - val_acc: 0.0137\n",
            "Epoch 130/150\n",
            "7155/7155 [==============================] - 4s 560us/step - loss: 4.7844 - acc: 0.0115 - val_loss: 4.7852 - val_acc: 0.0137\n",
            "Epoch 131/150\n",
            "7155/7155 [==============================] - 4s 551us/step - loss: 4.7849 - acc: 0.0089 - val_loss: 4.7852 - val_acc: 0.0137\n",
            "Epoch 132/150\n",
            "7155/7155 [==============================] - 4s 553us/step - loss: 4.7843 - acc: 0.0122 - val_loss: 4.7852 - val_acc: 0.0137\n",
            "Epoch 133/150\n",
            "7155/7155 [==============================] - 4s 553us/step - loss: 4.7843 - acc: 0.0120 - val_loss: 4.7852 - val_acc: 0.0137\n",
            "Epoch 134/150\n",
            "7155/7155 [==============================] - 4s 562us/step - loss: 4.7850 - acc: 0.0089 - val_loss: 4.7851 - val_acc: 0.0137\n",
            "Epoch 135/150\n",
            "7155/7155 [==============================] - 4s 548us/step - loss: 4.7844 - acc: 0.0095 - val_loss: 4.7851 - val_acc: 0.0137\n",
            "Epoch 136/150\n",
            "7155/7155 [==============================] - 4s 550us/step - loss: 4.7839 - acc: 0.0101 - val_loss: 4.7851 - val_acc: 0.0137\n",
            "Epoch 137/150\n",
            "7155/7155 [==============================] - 4s 544us/step - loss: 4.7844 - acc: 0.0115 - val_loss: 4.7851 - val_acc: 0.0137\n",
            "Epoch 138/150\n",
            "7155/7155 [==============================] - 4s 543us/step - loss: 4.7844 - acc: 0.0105 - val_loss: 4.7851 - val_acc: 0.0137\n",
            "Epoch 139/150\n",
            "7155/7155 [==============================] - 4s 543us/step - loss: 4.7846 - acc: 0.0106 - val_loss: 4.7851 - val_acc: 0.0137\n",
            "Epoch 140/150\n",
            "7155/7155 [==============================] - 4s 542us/step - loss: 4.7843 - acc: 0.0113 - val_loss: 4.7850 - val_acc: 0.0137\n",
            "Epoch 141/150\n",
            "7155/7155 [==============================] - 4s 548us/step - loss: 4.7841 - acc: 0.0115 - val_loss: 4.7850 - val_acc: 0.0137\n",
            "Epoch 142/150\n",
            "7155/7155 [==============================] - 4s 541us/step - loss: 4.7839 - acc: 0.0113 - val_loss: 4.7850 - val_acc: 0.0137\n",
            "Epoch 143/150\n",
            "7155/7155 [==============================] - 4s 544us/step - loss: 4.7840 - acc: 0.0106 - val_loss: 4.7850 - val_acc: 0.0137\n",
            "Epoch 144/150\n",
            "7155/7155 [==============================] - 4s 540us/step - loss: 4.7838 - acc: 0.0116 - val_loss: 4.7850 - val_acc: 0.0137\n",
            "Epoch 145/150\n",
            "7155/7155 [==============================] - 4s 544us/step - loss: 4.7835 - acc: 0.0138 - val_loss: 4.7850 - val_acc: 0.0137\n",
            "Epoch 146/150\n",
            "7155/7155 [==============================] - 4s 545us/step - loss: 4.7838 - acc: 0.0109 - val_loss: 4.7849 - val_acc: 0.0137\n",
            "Epoch 147/150\n",
            "7155/7155 [==============================] - 4s 541us/step - loss: 4.7842 - acc: 0.0113 - val_loss: 4.7849 - val_acc: 0.0137\n",
            "Epoch 148/150\n",
            "7155/7155 [==============================] - 4s 542us/step - loss: 4.7841 - acc: 0.0115 - val_loss: 4.7849 - val_acc: 0.0137\n",
            "Epoch 149/150\n",
            "7155/7155 [==============================] - 4s 543us/step - loss: 4.7841 - acc: 0.0112 - val_loss: 4.7849 - val_acc: 0.0137\n",
            "Epoch 150/150\n",
            "7155/7155 [==============================] - 4s 545us/step - loss: 4.7839 - acc: 0.0119 - val_loss: 4.7849 - val_acc: 0.0137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IxD8C8AghPv9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Changing Xception Base to improve the performance** "
      ]
    },
    {
      "metadata": {
        "id": "UmPHQZm3ohbY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model1 = Sequential()\n",
        "for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "model1.add(base_model)\n",
        "model1.add(GlobalAveragePooling2D())\n",
        "model1.add(Dense(num_class, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fyl6fca7pqm3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opt = optimizers.Adam(lr = learning_rate)\n",
        "model1.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xyM7CAO-p9le",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1784
        },
        "outputId": "50ea604c-d982-4e20-ba55-205038ef0896"
      },
      "cell_type": "code",
      "source": [
        "fit_m =model1.fit(x_train,y_train, batch_size=32, epochs=50,validation_data=(x_test,y_test),shuffle=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 7155 samples, validate on 3067 samples\n",
            "Epoch 1/50\n",
            "7155/7155 [==============================] - 49s 7ms/step - loss: 4.2824 - acc: 0.1126 - val_loss: 4.7871 - val_acc: 0.0065\n",
            "Epoch 2/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 3.3637 - acc: 0.3029 - val_loss: 4.7874 - val_acc: 0.0068\n",
            "Epoch 3/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 2.9725 - acc: 0.4017 - val_loss: 4.7871 - val_acc: 0.0072\n",
            "Epoch 4/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 2.6982 - acc: 0.4646 - val_loss: 4.7881 - val_acc: 0.0068\n",
            "Epoch 5/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 2.4865 - acc: 0.5188 - val_loss: 4.7881 - val_acc: 0.0068\n",
            "Epoch 6/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 2.3104 - acc: 0.5617 - val_loss: 4.7891 - val_acc: 0.0098\n",
            "Epoch 7/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 2.1752 - acc: 0.5943 - val_loss: 4.7898 - val_acc: 0.0068\n",
            "Epoch 8/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 2.0294 - acc: 0.6287 - val_loss: 4.7913 - val_acc: 0.0068\n",
            "Epoch 9/50\n",
            "7155/7155 [==============================] - 40s 6ms/step - loss: 1.9189 - acc: 0.6551 - val_loss: 4.7918 - val_acc: 0.0068\n",
            "Epoch 10/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 1.8172 - acc: 0.6820 - val_loss: 4.7931 - val_acc: 0.0068\n",
            "Epoch 11/50\n",
            "7155/7155 [==============================] - 40s 6ms/step - loss: 1.7299 - acc: 0.7052 - val_loss: 4.7935 - val_acc: 0.0068\n",
            "Epoch 12/50\n",
            "7155/7155 [==============================] - 40s 6ms/step - loss: 1.6630 - acc: 0.7216 - val_loss: 4.7943 - val_acc: 0.0068\n",
            "Epoch 13/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 1.5605 - acc: 0.7479 - val_loss: 4.7957 - val_acc: 0.0068\n",
            "Epoch 14/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 1.5032 - acc: 0.7572 - val_loss: 4.7969 - val_acc: 0.0068\n",
            "Epoch 15/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 1.4487 - acc: 0.7715 - val_loss: 4.7970 - val_acc: 0.0068\n",
            "Epoch 16/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 1.3825 - acc: 0.7870 - val_loss: 4.7983 - val_acc: 0.0068\n",
            "Epoch 17/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 1.3304 - acc: 0.7994 - val_loss: 4.7996 - val_acc: 0.0068\n",
            "Epoch 18/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 1.2665 - acc: 0.8164 - val_loss: 4.8001 - val_acc: 0.0068\n",
            "Epoch 19/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 1.2190 - acc: 0.8179 - val_loss: 4.8005 - val_acc: 0.0068\n",
            "Epoch 20/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 1.1739 - acc: 0.8338 - val_loss: 4.8029 - val_acc: 0.0068\n",
            "Epoch 21/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 1.1298 - acc: 0.8423 - val_loss: 4.8042 - val_acc: 0.0068\n",
            "Epoch 22/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 1.1120 - acc: 0.8496 - val_loss: 4.8052 - val_acc: 0.0068\n",
            "Epoch 23/50\n",
            "7155/7155 [==============================] - 40s 6ms/step - loss: 1.0634 - acc: 0.8512 - val_loss: 4.8064 - val_acc: 0.0068\n",
            "Epoch 24/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 1.0297 - acc: 0.8629 - val_loss: 4.8074 - val_acc: 0.0068\n",
            "Epoch 25/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.9994 - acc: 0.8637 - val_loss: 4.8084 - val_acc: 0.0068\n",
            "Epoch 26/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.9704 - acc: 0.8732 - val_loss: 4.8106 - val_acc: 0.0068\n",
            "Epoch 27/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 0.9306 - acc: 0.8780 - val_loss: 4.8127 - val_acc: 0.0068\n",
            "Epoch 28/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 0.9008 - acc: 0.8865 - val_loss: 4.8123 - val_acc: 0.0068\n",
            "Epoch 29/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 0.8795 - acc: 0.8894 - val_loss: 4.8142 - val_acc: 0.0068\n",
            "Epoch 30/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.8386 - acc: 0.8984 - val_loss: 4.8168 - val_acc: 0.0068\n",
            "Epoch 31/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.8276 - acc: 0.8978 - val_loss: 4.8167 - val_acc: 0.0068\n",
            "Epoch 32/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.7990 - acc: 0.9048 - val_loss: 4.8179 - val_acc: 0.0068\n",
            "Epoch 33/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 0.7711 - acc: 0.9065 - val_loss: 4.8189 - val_acc: 0.0068\n",
            "Epoch 34/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 0.7648 - acc: 0.9089 - val_loss: 4.8212 - val_acc: 0.0068\n",
            "Epoch 35/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 0.7253 - acc: 0.9150 - val_loss: 4.8218 - val_acc: 0.0068\n",
            "Epoch 36/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 0.7174 - acc: 0.9152 - val_loss: 4.8246 - val_acc: 0.0068\n",
            "Epoch 37/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.6966 - acc: 0.9170 - val_loss: 4.8260 - val_acc: 0.0068\n",
            "Epoch 38/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.7026 - acc: 0.9184 - val_loss: 4.8257 - val_acc: 0.0068\n",
            "Epoch 39/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.6571 - acc: 0.9259 - val_loss: 4.8292 - val_acc: 0.0068\n",
            "Epoch 40/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 0.6466 - acc: 0.9242 - val_loss: 4.8296 - val_acc: 0.0068\n",
            "Epoch 41/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 0.6278 - acc: 0.9325 - val_loss: 4.8316 - val_acc: 0.0068\n",
            "Epoch 42/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.6227 - acc: 0.9283 - val_loss: 4.8339 - val_acc: 0.0068\n",
            "Epoch 43/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.6072 - acc: 0.9335 - val_loss: 4.8337 - val_acc: 0.0068\n",
            "Epoch 44/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 0.5836 - acc: 0.9363 - val_loss: 4.8350 - val_acc: 0.0068\n",
            "Epoch 45/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 0.5543 - acc: 0.9406 - val_loss: 4.8365 - val_acc: 0.0068\n",
            "Epoch 46/50\n",
            "7155/7155 [==============================] - 42s 6ms/step - loss: 0.5626 - acc: 0.9386 - val_loss: 4.8384 - val_acc: 0.0068\n",
            "Epoch 47/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.5572 - acc: 0.9371 - val_loss: 4.8410 - val_acc: 0.0068\n",
            "Epoch 48/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.5463 - acc: 0.9396 - val_loss: 4.8411 - val_acc: 0.0068\n",
            "Epoch 49/50\n",
            "7155/7155 [==============================] - 40s 6ms/step - loss: 0.5397 - acc: 0.9409 - val_loss: 4.8406 - val_acc: 0.0068\n",
            "Epoch 50/50\n",
            "7155/7155 [==============================] - 41s 6ms/step - loss: 0.5296 - acc: 0.9426 - val_loss: 4.8433 - val_acc: 0.0068\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}